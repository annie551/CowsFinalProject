{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91fa8ed5",
   "metadata": {},
   "source": [
    "# XGBoost for Cattle Milk Yield Prediction\n",
    "\n",
    "This notebook implements nested 5-fold cross-validation for XGBoost to predict milk yield.\n",
    "- Uses nested CV (5-fold outer, 3-fold inner)\n",
    "- Uses entire dataset (no sampling)\n",
    "- Expanded hyperparameter grid: n_estimators 200-400, max_depth 3-21 (then more precise during further runs)\n",
    "- Saves models to models_xgb folder\n",
    "\n",
    "\n",
    "This code is very similar to modeling.ipynb, but it was specificaly for xgboost model. Steps all the way until model_creation is nearly identical. \n",
    "\n",
    "The differences are\n",
    "- larger parameter grid, now centered around teh optimal parameteres from preliminary testing\n",
    "- trains and cross validates on entire 210k dataset\n",
    "- provides inference for test data and produces an output submission.csv\n",
    "\n",
    "Since XGBoost trains much faster than comparable models, it was easier to have a larger grid and more finer tuned parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d57f2d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "from sklearn.model_selection import KFold, GridSearchCV, ParameterGrid\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from xgboost import XGBRegressor\n",
    "import joblib\n",
    "from tqdm import tqdm\n",
    "from itertools import product\n",
    "\n",
    "# Create models_xgb directory if it doesn't exist\n",
    "os.makedirs('models_xgb', exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "175aa02c",
   "metadata": {},
   "source": [
    "## Load and Prepare Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d5083b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (209926, 41)\n",
      "Test shape: (40000, 41)\n",
      "\n",
      "Train columns: ['Cattle_ID', 'Age_Months', 'Weight_kg', 'Parity', 'Lactation_Stage', 'Days_in_Milk', 'Feed_Type', 'Feed_Quantity_kg', 'Feeding_Frequency', 'Water_Intake_L', 'Walking_Distance_km', 'Grazing_Duration_hrs', 'Resting_Hours', 'Ambient_Temperature_C', 'Humidity_percent', 'Housing_Score', 'FMD_Vaccine', 'Brucellosis_Vaccine', 'HS_Vaccine', 'BQ_Vaccine', 'Anthrax_Vaccine', 'IBR_Vaccine', 'BVD_Vaccine', 'Rabies_Vaccine', 'Previous_Week_Avg_Yield', 'Body_Condition_Score', 'Milking_Interval_hrs', 'Farm_ID', 'Mastitis', 'Milk_Yield_L', 'Breed_Brown Swiss', 'Breed_Brown Swiss ', 'Breed_Guernsey', 'Breed_Holstein', 'Breed_Holstien', 'Breed_Jersey', 'Management_System_Intensive', 'Management_System_Mixed', 'Management_System_Pastoral', 'Management_System_Semi_Intensive', 'Date_Ordinal']\n"
     ]
    }
   ],
   "source": [
    "# Load cleaned data\n",
    "TRAIN_PATH = \"cleaned_train_nn.csv\"\n",
    "TEST_PATH = \"cleaned_test_nn.csv\"\n",
    "\n",
    "train = pd.read_csv(TRAIN_PATH)\n",
    "test = pd.read_csv(TEST_PATH)\n",
    "\n",
    "print(f\"Train shape: {train.shape}\")\n",
    "print(f\"Test shape: {test.shape}\")\n",
    "print(f\"\\nTrain columns: {train.columns.tolist()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "92a61c4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training features shape: (209926, 39)\n",
      "Training target shape: (209926,)\n",
      "Test features shape: (40000, 40)\n"
     ]
    }
   ],
   "source": [
    "# Separate features and target\n",
    "id_cols = ['Cattle_ID'] if 'Cattle_ID' in train.columns else []\n",
    "target_col = 'Milk_Yield_L'\n",
    "\n",
    "X_train = train.drop(columns=[target_col] + id_cols, errors='ignore')\n",
    "y_train = train[target_col]\n",
    "\n",
    "X_test = test.drop(columns=id_cols, errors='ignore')\n",
    "\n",
    "print(f\"Training features shape: {X_train.shape}\")\n",
    "print(f\"Training target shape: {y_train.shape}\")\n",
    "print(f\"Test features shape: {X_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e78c856",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical columns (1): ['Feed_Type']\n",
      "\n",
      "Numerical columns (28): ['Age_Months', 'Weight_kg', 'Parity', 'Lactation_Stage', 'Days_in_Milk', 'Feed_Quantity_kg', 'Feeding_Frequency', 'Water_Intake_L', 'Walking_Distance_km', 'Grazing_Duration_hrs', 'Resting_Hours', 'Ambient_Temperature_C', 'Humidity_percent', 'Housing_Score', 'FMD_Vaccine', 'Brucellosis_Vaccine', 'HS_Vaccine', 'BQ_Vaccine', 'Anthrax_Vaccine', 'IBR_Vaccine', 'BVD_Vaccine', 'Rabies_Vaccine', 'Previous_Week_Avg_Yield', 'Body_Condition_Score', 'Milking_Interval_hrs', 'Farm_ID', 'Mastitis', 'Date_Ordinal']\n"
     ]
    }
   ],
   "source": [
    "# Identify categorical and numerical columns\n",
    "categorical_cols = X_train.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "numerical_cols = X_train.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "\n",
    "print(f\"Categorical columns ({len(categorical_cols)}): {categorical_cols}\")\n",
    "print(f\"\\nNumerical columns ({len(numerical_cols)}): {numerical_cols}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f385957",
   "metadata": {},
   "source": [
    "## Preprocessing Pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ca19fc5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing pipeline created\n"
     ]
    }
   ],
   "source": [
    "# Create preprocessing pipeline for tree-based models (no scaling needed)\n",
    "preprocessor_tree = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', SimpleImputer(strategy='median'), numerical_cols),\n",
    "        ('cat', Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "            ('encoder', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "        ]), categorical_cols)\n",
    "    ],\n",
    "    remainder='drop'\n",
    ")\n",
    "\n",
    "print(\"Preprocessing pipeline created\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48445b16",
   "metadata": {},
   "source": [
    "## Nested Cross-Validation Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "adc1c567",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outer CV folds: 5\n",
      "Inner CV folds: 3\n",
      "Using entire dataset (no sampling)\n"
     ]
    }
   ],
   "source": [
    "# Setup nested CV\n",
    "OUTER_CV = 5\n",
    "INNER_CV = 3\n",
    "\n",
    "outer_cv = KFold(n_splits=OUTER_CV, shuffle=True, random_state=42)\n",
    "inner_cv = KFold(n_splits=INNER_CV, shuffle=True, random_state=42)\n",
    "\n",
    "print(f\"Outer CV folds: {OUTER_CV}\")\n",
    "print(f\"Inner CV folds: {INNER_CV}\")\n",
    "print(\"Using entire dataset (no sampling)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2199a038",
   "metadata": {},
   "source": [
    "## Hyperparameter Grid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "afbc69ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameter grid:\n",
      "  model__n_estimators: [200, 300, 400]\n",
      "  model__max_depth: [3, 4, 5]\n",
      "  model__learning_rate: [0.1]\n",
      "  model__subsample: [0.7, 0.75, 0.8]\n",
      "  model__colsample_bytree: [0.7, 0.75, 0.8]\n",
      "\n",
      "Total combinations: 81\n",
      "With 3-fold inner CV: 243 fits per outer fold\n"
     ]
    }
   ],
   "source": [
    "# Create pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor_tree),\n",
    "    ('model', XGBRegressor(random_state=42, n_jobs=-1, eval_metric='rmse'))\n",
    "])\n",
    "\n",
    "# Optimized hyperparameter grid (reduced size while keeping depth up to 21)\n",
    "param_grid = {\n",
    "    'model__n_estimators': [200, 300, 400],\n",
    "    'model__max_depth': [3, 4, 5],\n",
    "    'model__learning_rate': [0.1],\n",
    "    'model__subsample': [0.7, 0.75, 0.8],\n",
    "    'model__colsample_bytree': [0.7, 0.75, 0.8]\n",
    "}\n",
    "\n",
    "# Calculate grid size\n",
    "grid_size = 1\n",
    "for values in param_grid.values():\n",
    "    grid_size *= len(values)\n",
    "print(\"Hyperparameter grid:\")\n",
    "for key, values in param_grid.items():\n",
    "    print(f\"  {key}: {values}\")\n",
    "print(f\"\\nTotal combinations: {grid_size}\")\n",
    "print(f\"With {INNER_CV}-fold inner CV: {grid_size * INNER_CV} fits per outer fold\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eca255b",
   "metadata": {},
   "source": [
    "## Nested Cross-Validation Function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "047f2185",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nested_cv_evaluation(X, y, pipeline, param_grid, outer_cv, inner_cv):\n",
    "    \"\"\"\n",
    "    Perform nested cross-validation for XGBoost.\n",
    "    Saves each fold's best model to the models_xgb folder.\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Evaluating XGBoost\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Using entire dataset: {len(X):,} samples\")\n",
    "    \n",
    "    outer_scores = []\n",
    "    best_params_list = []\n",
    "    best_models = []\n",
    "    \n",
    "    for fold_idx, (train_idx, val_idx) in enumerate(outer_cv.split(X, y)):\n",
    "        print(f\"\\nOuter Fold {fold_idx + 1}/{outer_cv.n_splits}\")\n",
    "        \n",
    "        X_train_fold, X_val_fold = X.iloc[train_idx], X.iloc[val_idx]\n",
    "        y_train_fold, y_val_fold = y.iloc[train_idx], y.iloc[val_idx]\n",
    "        \n",
    "        # Calculate total number of fits for progress display\n",
    "        param_grid_list = list(ParameterGrid(param_grid))\n",
    "        n_candidates = len(param_grid_list)\n",
    "        n_fits = n_candidates * inner_cv.n_splits\n",
    "        \n",
    "        # Inner CV for hyperparameter tuning\n",
    "        print(f\"  Testing {n_candidates} parameter combinations ({n_fits} total fits)...\")\n",
    "        \n",
    "        grid_search = GridSearchCV(\n",
    "            pipeline,\n",
    "            param_grid,\n",
    "            cv=inner_cv,\n",
    "            scoring='neg_mean_squared_error',\n",
    "            n_jobs=-1,\n",
    "            verbose=1  # Use sklearn's built-in progress display\n",
    "        )\n",
    "        \n",
    "        start_time = time.time()\n",
    "        grid_search.fit(X_train_fold, y_train_fold)\n",
    "        fit_time = time.time() - start_time\n",
    "        \n",
    "        # Evaluate on validation set\n",
    "        y_pred = grid_search.predict(X_val_fold)\n",
    "        mse = mean_squared_error(y_val_fold, y_pred)\n",
    "        rmse = np.sqrt(mse)\n",
    "        mae = mean_absolute_error(y_val_fold, y_pred)\n",
    "        r2 = r2_score(y_val_fold, y_pred)\n",
    "        \n",
    "        outer_scores.append({\n",
    "            'mse': mse,\n",
    "            'rmse': rmse,\n",
    "            'mae': mae,\n",
    "            'r2': r2\n",
    "        })\n",
    "        \n",
    "        best_params_list.append(grid_search.best_params_)\n",
    "        best_models.append(grid_search.best_estimator_)\n",
    "        \n",
    "        # Save this fold's best model\n",
    "        model_path = f'models_xgb/xgboost_fold_{fold_idx + 1}.pkl'\n",
    "        joblib.dump(grid_search.best_estimator_, model_path)\n",
    "        print(f\"  Saved model to {model_path}\")\n",
    "        \n",
    "        print(f\"  Best params: {grid_search.best_params_}\")\n",
    "        print(f\"  Validation RMSE: {rmse:.4f}, MAE: {mae:.4f}, R²: {r2:.4f}\")\n",
    "        print(f\"  Fit time: {fit_time:.2f}s ({fit_time/60:.2f} minutes)\")\n",
    "    \n",
    "    # Aggregate results\n",
    "    avg_rmse = np.mean([s['rmse'] for s in outer_scores])\n",
    "    avg_mae = np.mean([s['mae'] for s in outer_scores])\n",
    "    avg_r2 = np.mean([s['r2'] for s in outer_scores])\n",
    "    std_rmse = np.std([s['rmse'] for s in outer_scores])\n",
    "    \n",
    "    print(f\"\\nXGBoost Results:\")\n",
    "    print(f\"  Average RMSE: {avg_rmse:.4f} (±{std_rmse:.4f})\")\n",
    "    print(f\"  Average MAE: {avg_mae:.4f}\")\n",
    "    print(f\"  Average R²: {avg_r2:.4f}\")\n",
    "    \n",
    "    return {\n",
    "        'outer_scores': outer_scores,\n",
    "        'best_params': best_params_list,\n",
    "        'best_models': best_models,\n",
    "        'avg_rmse': avg_rmse,\n",
    "        'avg_mae': avg_mae,\n",
    "        'avg_r2': avg_r2,\n",
    "        'std_rmse': std_rmse\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b677497d",
   "metadata": {},
   "source": [
    "## Run Nested Cross-Validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c2d8c227",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Evaluating XGBoost\n",
      "============================================================\n",
      "Using entire dataset: 209,926 samples\n",
      "\n",
      "Outer Fold 1/5\n",
      "  Testing 81 parameter combinations (243 total fits)...\n",
      "Fitting 3 folds for each of 81 candidates, totalling 243 fits\n",
      "  Saved model to models_xgb/xgboost_fold_1.pkl\n",
      "  Best params: {'model__colsample_bytree': 0.75, 'model__learning_rate': 0.1, 'model__max_depth': 3, 'model__n_estimators': 300, 'model__subsample': 0.7}\n",
      "  Validation RMSE: 4.1200, MAE: 3.1991, R²: 0.4088\n",
      "  Fit time: 61.94s (1.03 minutes)\n",
      "\n",
      "Outer Fold 2/5\n",
      "  Testing 81 parameter combinations (243 total fits)...\n",
      "Fitting 3 folds for each of 81 candidates, totalling 243 fits\n",
      "  Saved model to models_xgb/xgboost_fold_2.pkl\n",
      "  Best params: {'model__colsample_bytree': 0.7, 'model__learning_rate': 0.1, 'model__max_depth': 3, 'model__n_estimators': 300, 'model__subsample': 0.75}\n",
      "  Validation RMSE: 4.1086, MAE: 3.2044, R²: 0.4084\n",
      "  Fit time: 60.58s (1.01 minutes)\n",
      "\n",
      "Outer Fold 3/5\n",
      "  Testing 81 parameter combinations (243 total fits)...\n",
      "Fitting 3 folds for each of 81 candidates, totalling 243 fits\n",
      "  Saved model to models_xgb/xgboost_fold_3.pkl\n",
      "  Best params: {'model__colsample_bytree': 0.8, 'model__learning_rate': 0.1, 'model__max_depth': 3, 'model__n_estimators': 300, 'model__subsample': 0.7}\n",
      "  Validation RMSE: 4.1286, MAE: 3.2121, R²: 0.4032\n",
      "  Fit time: 63.13s (1.05 minutes)\n",
      "\n",
      "Outer Fold 4/5\n",
      "  Testing 81 parameter combinations (243 total fits)...\n",
      "Fitting 3 folds for each of 81 candidates, totalling 243 fits\n",
      "  Saved model to models_xgb/xgboost_fold_4.pkl\n",
      "  Best params: {'model__colsample_bytree': 0.75, 'model__learning_rate': 0.1, 'model__max_depth': 3, 'model__n_estimators': 300, 'model__subsample': 0.75}\n",
      "  Validation RMSE: 4.1145, MAE: 3.1988, R²: 0.4092\n",
      "  Fit time: 62.21s (1.04 minutes)\n",
      "\n",
      "Outer Fold 5/5\n",
      "  Testing 81 parameter combinations (243 total fits)...\n",
      "Fitting 3 folds for each of 81 candidates, totalling 243 fits\n",
      "  Saved model to models_xgb/xgboost_fold_5.pkl\n",
      "  Best params: {'model__colsample_bytree': 0.75, 'model__learning_rate': 0.1, 'model__max_depth': 3, 'model__n_estimators': 300, 'model__subsample': 0.75}\n",
      "  Validation RMSE: 4.1101, MAE: 3.1927, R²: 0.4029\n",
      "  Fit time: 66.49s (1.11 minutes)\n",
      "\n",
      "XGBoost Results:\n",
      "  Average RMSE: 4.1164 (±0.0073)\n",
      "  Average MAE: 3.2014\n",
      "  Average R²: 0.4065\n",
      "\n",
      "Total time: 5.24 minutes (0.09 hours)\n"
     ]
    }
   ],
   "source": [
    "# Run nested CV\n",
    "start_time = time.time()\n",
    "\n",
    "results = nested_cv_evaluation(\n",
    "    X_train, y_train,\n",
    "    pipeline,\n",
    "    param_grid,\n",
    "    outer_cv,\n",
    "    inner_cv\n",
    ")\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "print(f\"\\nTotal time: {total_time/60:.2f} minutes ({total_time/3600:.2f} hours)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d1838f",
   "metadata": {},
   "source": [
    "## Results Summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ab09a13b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "RESULTS SUMMARY\n",
      "============================================================\n",
      "Average RMSE: 4.1164 (±0.0073)\n",
      "Average MAE: 3.2014\n",
      "Average R²: 0.4065\n",
      "\n",
      "Best parameters for each fold:\n",
      "\n",
      "Fold 1:\n",
      "  model__colsample_bytree: 0.75\n",
      "  model__learning_rate: 0.1\n",
      "  model__max_depth: 3\n",
      "  model__n_estimators: 300\n",
      "  model__subsample: 0.7\n",
      "\n",
      "Fold 2:\n",
      "  model__colsample_bytree: 0.7\n",
      "  model__learning_rate: 0.1\n",
      "  model__max_depth: 3\n",
      "  model__n_estimators: 300\n",
      "  model__subsample: 0.75\n",
      "\n",
      "Fold 3:\n",
      "  model__colsample_bytree: 0.8\n",
      "  model__learning_rate: 0.1\n",
      "  model__max_depth: 3\n",
      "  model__n_estimators: 300\n",
      "  model__subsample: 0.7\n",
      "\n",
      "Fold 4:\n",
      "  model__colsample_bytree: 0.75\n",
      "  model__learning_rate: 0.1\n",
      "  model__max_depth: 3\n",
      "  model__n_estimators: 300\n",
      "  model__subsample: 0.75\n",
      "\n",
      "Fold 5:\n",
      "  model__colsample_bytree: 0.75\n",
      "  model__learning_rate: 0.1\n",
      "  model__max_depth: 3\n",
      "  model__n_estimators: 300\n",
      "  model__subsample: 0.75\n",
      "\n",
      "============================================================\n",
      "MOST COMMON BEST PARAMETERS (across all folds):\n",
      "============================================================\n",
      "  model__colsample_bytree: 0.75\n",
      "  model__learning_rate: 0.1\n",
      "  model__max_depth: 3\n",
      "  model__n_estimators: 300\n",
      "  model__subsample: 0.75\n"
     ]
    }
   ],
   "source": [
    "# Display results summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"RESULTS SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Average RMSE: {results['avg_rmse']:.4f} (±{results['std_rmse']:.4f})\")\n",
    "print(f\"Average MAE: {results['avg_mae']:.4f}\")\n",
    "print(f\"Average R²: {results['avg_r2']:.4f}\")\n",
    "\n",
    "print(\"\\nBest parameters for each fold:\")\n",
    "for i, params in enumerate(results['best_params']):\n",
    "    print(f\"\\nFold {i+1}:\")\n",
    "    for key, value in params.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "\n",
    "# Get most common best parameters across folds\n",
    "def get_most_common_params(best_params_list):\n",
    "    \"\"\"Get the most common parameter values across folds\"\"\"\n",
    "    if not best_params_list:\n",
    "        return {}\n",
    "    \n",
    "    param_counts = {}\n",
    "    for params in best_params_list:\n",
    "        for key, value in params.items():\n",
    "            if key not in param_counts:\n",
    "                param_counts[key] = {}\n",
    "            if value not in param_counts[key]:\n",
    "                param_counts[key][value] = 0\n",
    "            param_counts[key][value] += 1\n",
    "    \n",
    "    most_common = {}\n",
    "    for key, value_counts in param_counts.items():\n",
    "        most_common[key] = max(value_counts, key=value_counts.get)\n",
    "    \n",
    "    return most_common\n",
    "\n",
    "best_params_final = get_most_common_params(results['best_params'])\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MOST COMMON BEST PARAMETERS (across all folds):\")\n",
    "print(\"=\"*60)\n",
    "for key, value in best_params_final.items():\n",
    "    print(f\"  {key}: {value}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07f08cf2",
   "metadata": {},
   "source": [
    "## Retrain on Entire Dataset with Best Parameters\n",
    "\n",
    "We then pick the best parameters to be the most common parameters (mode) out of all 5 folds, and then we retrain a model on the entire dataset (no train/test split).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "97cb88e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retraining on entire dataset with best hyperparameters...\n",
      "Training samples: 209,926\n",
      "Training completed in 1.28 seconds (0.02 minutes)\n",
      "\n",
      "Final Model Training Performance:\n",
      "  RMSE: 4.0825\n",
      "  MAE: 3.1756\n",
      "  R²: 0.4163\n",
      "\n",
      "Final model saved to models_xgb/xgboost_final.pkl\n"
     ]
    }
   ],
   "source": [
    "# Create final pipeline with best parameters\n",
    "final_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor_tree),\n",
    "    ('model', XGBRegressor(random_state=42, n_jobs=-1, eval_metric='rmse'))\n",
    "])\n",
    "\n",
    "# Set best parameters\n",
    "final_pipeline.set_params(**best_params_final)\n",
    "\n",
    "print(\"Retraining on entire dataset with best hyperparameters...\")\n",
    "print(f\"Training samples: {len(X_train):,}\")\n",
    "\n",
    "start_time = time.time()\n",
    "final_pipeline.fit(X_train, y_train)\n",
    "fit_time = time.time() - start_time\n",
    "\n",
    "print(f\"Training completed in {fit_time:.2f} seconds ({fit_time/60:.2f} minutes)\")\n",
    "\n",
    "# Evaluate on training data\n",
    "y_train_pred = final_pipeline.predict(X_train)\n",
    "train_mse = mean_squared_error(y_train, y_train_pred)\n",
    "train_rmse = np.sqrt(train_mse)\n",
    "train_mae = mean_absolute_error(y_train, y_train_pred)\n",
    "train_r2 = r2_score(y_train, y_train_pred)\n",
    "\n",
    "print(\"\\nFinal Model Training Performance:\")\n",
    "print(f\"  RMSE: {train_rmse:.4f}\")\n",
    "print(f\"  MAE: {train_mae:.4f}\")\n",
    "print(f\"  R²: {train_r2:.4f}\")\n",
    "\n",
    "# Save final model\n",
    "final_model_path = 'models_xgb/xgboost_final.pkl'\n",
    "joblib.dump(final_pipeline, final_model_path)\n",
    "print(f\"\\nFinal model saved to {final_model_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7076b4d",
   "metadata": {},
   "source": [
    "## Generate Predictions on Test Data\n",
    "\n",
    "\n",
    "We then produce the output to the submission_xgboost.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9db6a637",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating predictions on test data...\n",
      "Predictions shape: (40000,)\n",
      "Predictions range: [4.38, 27.67]\n",
      "Predictions mean: 15.60\n",
      "Predictions std: 3.36\n",
      "\n",
      "Submission file saved to: submission_xgboost.csv\n",
      "Submission shape: (40000, 2)\n",
      "\n",
      "First few predictions:\n",
      "   Cattle_ID  Milk_Yield_L\n",
      "0          1     18.817492\n",
      "1          2     10.555753\n",
      "2          3     22.754541\n",
      "3          4     15.351189\n",
      "4          5     18.558693\n",
      "5          6     19.449032\n",
      "6          7     15.052192\n",
      "7          8     18.379040\n",
      "8          9     21.967062\n",
      "9         10     14.004067\n"
     ]
    }
   ],
   "source": [
    "# Generate predictions on test data using final model\n",
    "print(\"Generating predictions on test data...\")\n",
    "test_predictions = final_pipeline.predict(X_test)\n",
    "\n",
    "print(f\"Predictions shape: {test_predictions.shape}\")\n",
    "print(f\"Predictions range: [{test_predictions.min():.2f}, {test_predictions.max():.2f}]\")\n",
    "print(f\"Predictions mean: {test_predictions.mean():.2f}\")\n",
    "print(f\"Predictions std: {test_predictions.std():.2f}\")\n",
    "\n",
    "# Create submission file\n",
    "if 'Cattle_ID' in test.columns:\n",
    "    submission = pd.DataFrame({\n",
    "        'Cattle_ID': test['Cattle_ID'],\n",
    "        'Milk_Yield_L': test_predictions\n",
    "    })\n",
    "else:\n",
    "    # If no Cattle_ID, create sequential IDs\n",
    "    submission = pd.DataFrame({\n",
    "        'Cattle_ID': range(1, len(test_predictions) + 1),\n",
    "        'Milk_Yield_L': test_predictions\n",
    "    })\n",
    "\n",
    "# Save submission\n",
    "submission_path = 'submission_xgboost.csv'\n",
    "submission.to_csv(submission_path, index=False)\n",
    "print(f\"\\nSubmission file saved to: {submission_path}\")\n",
    "print(f\"Submission shape: {submission.shape}\")\n",
    "print(\"\\nFirst few predictions:\")\n",
    "print(submission.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a82274",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
