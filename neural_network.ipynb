{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09b6beec",
   "metadata": {},
   "source": [
    "# PyTorch Neural Network for Cattle Milk Yield Prediction\n",
    "\n",
    "This notebook implements nested cross-validation for PyTorch neural networks to predict milk yield.\n",
    "- Uses MPS (Metal Performance Shaders) for GPU acceleration on Apple Silicon\n",
    "- Uses entire dataset (no sampling)\n",
    "- Saves models to models_nn folder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "08475097",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MPS (Metal Performance Shaders)\n",
      "Device: mps\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from itertools import product\n",
    "import joblib\n",
    "\n",
    "# Create models_nn directory if it doesn't exist\n",
    "os.makedirs('models_nn', exist_ok=True)\n",
    "\n",
    "# Set device (MPS for Apple Silicon, fallback to CPU)\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"Using MPS (Metal Performance Shaders)\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"Using CUDA\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Using CPU\")\n",
    "\n",
    "print(f\"Device: {device}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea759fc4",
   "metadata": {},
   "source": [
    "## Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a347be7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (209926, 41)\n",
      "Test shape: (40000, 41)\n",
      "\n",
      "Train columns: ['Cattle_ID', 'Age_Months', 'Weight_kg', 'Parity', 'Lactation_Stage', 'Days_in_Milk', 'Feed_Type', 'Feed_Quantity_kg', 'Feeding_Frequency', 'Water_Intake_L', 'Walking_Distance_km', 'Grazing_Duration_hrs', 'Resting_Hours', 'Ambient_Temperature_C', 'Humidity_percent', 'Housing_Score', 'FMD_Vaccine', 'Brucellosis_Vaccine', 'HS_Vaccine', 'BQ_Vaccine', 'Anthrax_Vaccine', 'IBR_Vaccine', 'BVD_Vaccine', 'Rabies_Vaccine', 'Previous_Week_Avg_Yield', 'Body_Condition_Score', 'Milking_Interval_hrs', 'Farm_ID', 'Mastitis', 'Milk_Yield_L', 'Breed_Brown Swiss', 'Breed_Brown Swiss ', 'Breed_Guernsey', 'Breed_Holstein', 'Breed_Holstien', 'Breed_Jersey', 'Management_System_Intensive', 'Management_System_Mixed', 'Management_System_Pastoral', 'Management_System_Semi_Intensive', 'Date_Ordinal']\n"
     ]
    }
   ],
   "source": [
    "# Load cleaned data\n",
    "TRAIN_PATH = \"cleaned_train_nn.csv\"\n",
    "TEST_PATH = \"cleaned_test_nn.csv\"\n",
    "\n",
    "train = pd.read_csv(TRAIN_PATH)\n",
    "test = pd.read_csv(TEST_PATH)\n",
    "\n",
    "print(f\"Train shape: {train.shape}\")\n",
    "print(f\"Test shape: {test.shape}\")\n",
    "print(f\"\\nTrain columns: {train.columns.tolist()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f078ae0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training features shape: (209926, 39)\n",
      "Training target shape: (209926,)\n",
      "Test features shape: (40000, 40)\n"
     ]
    }
   ],
   "source": [
    "# Separate features and target\n",
    "id_cols = ['Cattle_ID'] if 'Cattle_ID' in train.columns else []\n",
    "target_col = 'Milk_Yield_L'\n",
    "\n",
    "X_train = train.drop(columns=[target_col] + id_cols, errors='ignore')\n",
    "y_train = train[target_col]\n",
    "\n",
    "X_test = test.drop(columns=id_cols, errors='ignore')\n",
    "\n",
    "print(f\"Training features shape: {X_train.shape}\")\n",
    "print(f\"Training target shape: {y_train.shape}\")\n",
    "print(f\"Test features shape: {X_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "76b4a371",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical columns (1): ['Feed_Type']\n",
      "\n",
      "Numerical columns (28): ['Age_Months', 'Weight_kg', 'Parity', 'Lactation_Stage', 'Days_in_Milk', 'Feed_Quantity_kg', 'Feeding_Frequency', 'Water_Intake_L', 'Walking_Distance_km', 'Grazing_Duration_hrs', 'Resting_Hours', 'Ambient_Temperature_C', 'Humidity_percent', 'Housing_Score', 'FMD_Vaccine', 'Brucellosis_Vaccine', 'HS_Vaccine', 'BQ_Vaccine', 'Anthrax_Vaccine', 'IBR_Vaccine', 'BVD_Vaccine', 'Rabies_Vaccine', 'Previous_Week_Avg_Yield', 'Body_Condition_Score', 'Milking_Interval_hrs', 'Farm_ID', 'Mastitis', 'Date_Ordinal']\n"
     ]
    }
   ],
   "source": [
    "# Identify categorical and numerical columns\n",
    "categorical_cols = X_train.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "numerical_cols = X_train.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "\n",
    "print(f\"Categorical columns ({len(categorical_cols)}): {categorical_cols}\")\n",
    "print(f\"\\nNumerical columns ({len(numerical_cols)}): {numerical_cols}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e57c522",
   "metadata": {},
   "source": [
    "## Preprocessing Pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a51fb8e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing pipeline created\n"
     ]
    }
   ],
   "source": [
    "# Create preprocessing pipeline (same as modeling.ipynb)\n",
    "preprocessor_scaled = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='median')),\n",
    "            ('scaler', StandardScaler())\n",
    "        ]), numerical_cols),\n",
    "        ('cat', Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "            ('encoder', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "        ]), categorical_cols)\n",
    "    ],\n",
    "    remainder='drop'\n",
    ")\n",
    "\n",
    "print(\"Preprocessing pipeline created\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "238fd300",
   "metadata": {},
   "source": [
    "## PyTorch Dataset and Model Definitions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fbac3979",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch Dataset class\n",
    "class CattleDataset(Dataset):\n",
    "    def __init__(self, X, y=None):\n",
    "        self.X = torch.FloatTensor(X)\n",
    "        self.y = torch.FloatTensor(y) if y is not None else None\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if self.y is not None:\n",
    "            return self.X[idx], self.y[idx]\n",
    "        return self.X[idx]\n",
    "\n",
    "# Neural Network Model\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_size, hidden_sizes, activation='relu', dropout=0.0):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        \n",
    "        layers = []\n",
    "        prev_size = input_size\n",
    "        \n",
    "        for hidden_size in hidden_sizes:\n",
    "            layers.append(nn.Linear(prev_size, hidden_size))\n",
    "            if activation == 'relu':\n",
    "                layers.append(nn.ReLU())\n",
    "            elif activation == 'tanh':\n",
    "                layers.append(nn.Tanh())\n",
    "            if dropout > 0:\n",
    "                layers.append(nn.Dropout(dropout))\n",
    "            prev_size = hidden_size\n",
    "        \n",
    "        # Output layer (regression)\n",
    "        layers.append(nn.Linear(prev_size, 1))\n",
    "        \n",
    "        self.network = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.network(x).squeeze()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ec1a60",
   "metadata": {},
   "source": [
    "## Training Function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3996b07c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, learning_rate, weight_decay, epochs, device):\n",
    "    \"\"\"Train a PyTorch neural network model\"\"\"\n",
    "    model = model.to(device)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    patience = 20\n",
    "    patience_counter = 0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in val_loader:\n",
    "                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "                outputs = model(X_batch)\n",
    "                loss = criterion(outputs, y_batch)\n",
    "                val_loss += loss.item()\n",
    "        \n",
    "        train_loss /= len(train_loader)\n",
    "        val_loss /= len(val_loader)\n",
    "        \n",
    "        # Early stopping\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                break\n",
    "    \n",
    "    return model, best_val_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1720acbe",
   "metadata": {},
   "source": [
    "## Nested Cross-Validation Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b1246504",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outer CV folds: 5\n",
      "Inner CV folds: 3\n",
      "Using entire dataset (no sampling)\n"
     ]
    }
   ],
   "source": [
    "# Setup nested CV\n",
    "OUTER_CV = 5\n",
    "INNER_CV = 3\n",
    "\n",
    "outer_cv = KFold(n_splits=OUTER_CV, shuffle=True, random_state=42)\n",
    "inner_cv = KFold(n_splits=INNER_CV, shuffle=True, random_state=42)\n",
    "\n",
    "print(f\"Outer CV folds: {OUTER_CV}\")\n",
    "print(f\"Inner CV folds: {INNER_CV}\")\n",
    "print(\"Using entire dataset (no sampling)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa88c08",
   "metadata": {},
   "source": [
    "## Hyperparameter Grid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f90e4c78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameter grid:\n",
      "  hidden_layer_sizes: [(50,), (100,), (100, 50), (200, 100)]\n",
      "  alpha: [0.0001, 0.001, 0.01]\n",
      "  learning_rate: [0.001, 0.01]\n",
      "  activation: ['relu', 'tanh']\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameter grid (similar to sklearn MLPRegressor)\n",
    "param_grid = {\n",
    "    'hidden_layer_sizes': [(50,), (100,), (100, 50), (200, 100)],\n",
    "    'alpha': [0.0001, 0.001, 0.01],  # weight_decay (L2 regularization)\n",
    "    'learning_rate': [0.001, 0.01],\n",
    "    'activation': ['relu', 'tanh']\n",
    "}\n",
    "\n",
    "print(\"Hyperparameter grid:\")\n",
    "for key, values in param_grid.items():\n",
    "    print(f\"  {key}: {values}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aea3a05",
   "metadata": {},
   "source": [
    "## Nested Cross-Validation Function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "898831a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nested_cv_evaluation_pytorch(X, y, preprocessor, param_grid, outer_cv, inner_cv, device, batch_size=512, epochs=200):\n",
    "    \"\"\"\n",
    "    Perform nested cross-validation for PyTorch neural network.\n",
    "    Saves each fold's best model to the models_nn folder.\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Evaluating PyTorch Neural Network\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Using entire dataset: {len(X):,} samples\")\n",
    "    \n",
    "    # Fit preprocessor on full data to get feature dimension\n",
    "    X_processed = preprocessor.fit_transform(X)\n",
    "    input_size = X_processed.shape[1]\n",
    "    print(f\"Input feature size after preprocessing: {input_size}\")\n",
    "    \n",
    "    outer_scores = []\n",
    "    best_params_list = []\n",
    "    best_models = []\n",
    "    \n",
    "    for fold_idx, (train_idx, val_idx) in enumerate(outer_cv.split(X, y)):\n",
    "        print(f\"\\nOuter Fold {fold_idx + 1}/{outer_cv.n_splits}\")\n",
    "        \n",
    "        X_train_fold, X_val_fold = X.iloc[train_idx], X.iloc[val_idx]\n",
    "        y_train_fold, y_val_fold = y.iloc[train_idx], y.iloc[val_idx]\n",
    "        \n",
    "        # Fit preprocessor on training fold\n",
    "        X_train_processed = preprocessor.fit_transform(X_train_fold)\n",
    "        X_val_processed = preprocessor.transform(X_val_fold)\n",
    "        \n",
    "        # Inner CV for hyperparameter tuning\n",
    "        best_inner_score = float('inf')\n",
    "        best_inner_params = None\n",
    "        best_inner_model = None\n",
    "        \n",
    "        # Generate all parameter combinations\n",
    "        param_combinations = list(product(*param_grid.values()))\n",
    "        param_keys = list(param_grid.keys())\n",
    "        \n",
    "        print(f\"  Testing {len(param_combinations)} hyperparameter combinations...\")\n",
    "        \n",
    "        for param_idx, param_values in enumerate(param_combinations):\n",
    "            params = dict(zip(param_keys, param_values))\n",
    "            \n",
    "            inner_scores = []\n",
    "            \n",
    "            for inner_train_idx, inner_val_idx in inner_cv.split(X_train_fold, y_train_fold):\n",
    "                X_inner_train = X_train_processed[inner_train_idx]\n",
    "                X_inner_val = X_train_processed[inner_val_idx]\n",
    "                y_inner_train = y_train_fold.iloc[inner_train_idx].values\n",
    "                y_inner_val = y_train_fold.iloc[inner_val_idx].values\n",
    "                \n",
    "                # Create datasets and dataloaders\n",
    "                train_dataset = CattleDataset(X_inner_train, y_inner_train)\n",
    "                val_dataset = CattleDataset(X_inner_val, y_inner_val)\n",
    "                train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "                val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "                \n",
    "                # Create and train model\n",
    "                model = NeuralNetwork(\n",
    "                    input_size=input_size,\n",
    "                    hidden_sizes=params['hidden_layer_sizes'],\n",
    "                    activation=params['activation'],\n",
    "                    dropout=0.0\n",
    "                )\n",
    "                \n",
    "                _, val_loss = train_model(\n",
    "                    model, train_loader, val_loader,\n",
    "                    learning_rate=params['learning_rate'],\n",
    "                    weight_decay=params['alpha'],\n",
    "                    epochs=epochs,\n",
    "                    device=device\n",
    "                )\n",
    "                \n",
    "                inner_scores.append(val_loss)\n",
    "            \n",
    "            avg_inner_score = np.mean(inner_scores)\n",
    "            \n",
    "            if avg_inner_score < best_inner_score:\n",
    "                best_inner_score = avg_inner_score\n",
    "                best_inner_params = params\n",
    "            \n",
    "            if (param_idx + 1) % 5 == 0:\n",
    "                print(f\"    Tested {param_idx + 1}/{len(param_combinations)} combinations...\")\n",
    "        \n",
    "        # Train best model on full training fold with best parameters\n",
    "        print(f\"  Best hyperparameters found. Training final model...\")\n",
    "        train_dataset = CattleDataset(X_train_processed, y_train_fold.values)\n",
    "        val_dataset = CattleDataset(X_val_processed, y_val_fold.values)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "        \n",
    "        best_inner_model = NeuralNetwork(\n",
    "            input_size=input_size,\n",
    "            hidden_sizes=best_inner_params['hidden_layer_sizes'],\n",
    "            activation=best_inner_params['activation'],\n",
    "            dropout=0.0\n",
    "        )\n",
    "        best_inner_model, _ = train_model(\n",
    "            best_inner_model, train_loader, val_loader,\n",
    "            learning_rate=best_inner_params['learning_rate'],\n",
    "            weight_decay=best_inner_params['alpha'],\n",
    "            epochs=epochs,\n",
    "            device=device\n",
    "        )\n",
    "        \n",
    "        # Evaluate best model on outer validation set\n",
    "        best_inner_model.eval()\n",
    "        with torch.no_grad():\n",
    "            X_val_tensor = torch.FloatTensor(X_val_processed).to(device)\n",
    "            y_pred = best_inner_model(X_val_tensor).cpu().numpy()\n",
    "        \n",
    "        mse = mean_squared_error(y_val_fold, y_pred)\n",
    "        rmse = np.sqrt(mse)\n",
    "        mae = mean_absolute_error(y_val_fold, y_pred)\n",
    "        r2 = r2_score(y_val_fold, y_pred)\n",
    "        \n",
    "        outer_scores.append({\n",
    "            'mse': mse,\n",
    "            'rmse': rmse,\n",
    "            'mae': mae,\n",
    "            'r2': r2\n",
    "        })\n",
    "        \n",
    "        best_params_list.append(best_inner_params)\n",
    "        best_models.append((best_inner_model, preprocessor))\n",
    "        \n",
    "        # Save this fold's best model and preprocessor\n",
    "        model_path = f'models_nn/neuralnetwork_fold_{fold_idx + 1}.pth'\n",
    "        preprocessor_path = f'models_nn/preprocessor_fold_{fold_idx + 1}.pkl'\n",
    "        \n",
    "        torch.save({\n",
    "            'model_state_dict': best_inner_model.state_dict(),\n",
    "            'model_params': {\n",
    "                'input_size': input_size,\n",
    "                'hidden_sizes': best_inner_params['hidden_layer_sizes'],\n",
    "                'activation': best_inner_params['activation']\n",
    "            },\n",
    "            'hyperparams': best_inner_params\n",
    "        }, model_path)\n",
    "        \n",
    "        joblib.dump(preprocessor, preprocessor_path)\n",
    "        \n",
    "        print(f\"  Saved model to {model_path}\")\n",
    "        print(f\"  Saved preprocessor to {preprocessor_path}\")\n",
    "        print(f\"  Best params: {best_inner_params}\")\n",
    "        print(f\"  Validation RMSE: {rmse:.4f}, MAE: {mae:.4f}, R²: {r2:.4f}\")\n",
    "    \n",
    "    # Aggregate results\n",
    "    avg_rmse = np.mean([s['rmse'] for s in outer_scores])\n",
    "    avg_mae = np.mean([s['mae'] for s in outer_scores])\n",
    "    avg_r2 = np.mean([s['r2'] for s in outer_scores])\n",
    "    std_rmse = np.std([s['rmse'] for s in outer_scores])\n",
    "    \n",
    "    print(f\"\\nPyTorch Neural Network Results:\")\n",
    "    print(f\"  Average RMSE: {avg_rmse:.4f} (±{std_rmse:.4f})\")\n",
    "    print(f\"  Average MAE: {avg_mae:.4f}\")\n",
    "    print(f\"  Average R²: {avg_r2:.4f}\")\n",
    "    \n",
    "    return {\n",
    "        'outer_scores': outer_scores,\n",
    "        'best_params': best_params_list,\n",
    "        'best_models': best_models,\n",
    "        'avg_rmse': avg_rmse,\n",
    "        'avg_mae': avg_mae,\n",
    "        'avg_r2': avg_r2,\n",
    "        'std_rmse': std_rmse\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e12abbe4",
   "metadata": {},
   "source": [
    "## Run Nested Cross-Validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "204f9677",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Evaluating PyTorch Neural Network\n",
      "============================================================\n",
      "Using entire dataset: 209,926 samples\n",
      "Input feature size after preprocessing: 36\n",
      "\n",
      "Outer Fold 1/5\n",
      "  Testing 48 hyperparameter combinations...\n",
      "    Tested 5/48 combinations...\n",
      "    Tested 10/48 combinations...\n",
      "    Tested 15/48 combinations...\n",
      "    Tested 20/48 combinations...\n",
      "    Tested 25/48 combinations...\n",
      "    Tested 30/48 combinations...\n",
      "    Tested 35/48 combinations...\n",
      "    Tested 40/48 combinations...\n",
      "    Tested 45/48 combinations...\n",
      "  Best hyperparameters found. Training final model...\n",
      "  Saved model to models_nn/neuralnetwork_fold_1.pth\n",
      "  Saved preprocessor to models_nn/preprocessor_fold_1.pkl\n",
      "  Best params: {'hidden_layer_sizes': (200, 100), 'alpha': 0.01, 'learning_rate': 0.001, 'activation': 'tanh'}\n",
      "  Validation RMSE: 4.2592, MAE: 3.3082, R²: 0.3682\n",
      "\n",
      "Outer Fold 2/5\n",
      "  Testing 48 hyperparameter combinations...\n",
      "    Tested 5/48 combinations...\n",
      "    Tested 10/48 combinations...\n",
      "    Tested 15/48 combinations...\n",
      "    Tested 20/48 combinations...\n",
      "    Tested 25/48 combinations...\n",
      "    Tested 30/48 combinations...\n",
      "    Tested 35/48 combinations...\n",
      "    Tested 40/48 combinations...\n",
      "    Tested 45/48 combinations...\n",
      "  Best hyperparameters found. Training final model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/arnavjain/School/Year3Fall/CS363M/CowsFinalProject/.venv/lib/python3.12/site-packages/torch/nn/modules/loss.py:634: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Saved model to models_nn/neuralnetwork_fold_2.pth\n",
      "  Saved preprocessor to models_nn/preprocessor_fold_2.pkl\n",
      "  Best params: {'hidden_layer_sizes': (200, 100), 'alpha': 0.01, 'learning_rate': 0.001, 'activation': 'tanh'}\n",
      "  Validation RMSE: 4.2490, MAE: 3.3099, R²: 0.3673\n",
      "\n",
      "Outer Fold 3/5\n",
      "  Testing 48 hyperparameter combinations...\n",
      "    Tested 5/48 combinations...\n",
      "    Tested 10/48 combinations...\n",
      "    Tested 15/48 combinations...\n",
      "    Tested 20/48 combinations...\n",
      "    Tested 25/48 combinations...\n",
      "    Tested 30/48 combinations...\n",
      "    Tested 35/48 combinations...\n",
      "    Tested 40/48 combinations...\n",
      "    Tested 45/48 combinations...\n",
      "  Best hyperparameters found. Training final model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/arnavjain/School/Year3Fall/CS363M/CowsFinalProject/.venv/lib/python3.12/site-packages/torch/nn/modules/loss.py:634: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Saved model to models_nn/neuralnetwork_fold_3.pth\n",
      "  Saved preprocessor to models_nn/preprocessor_fold_3.pkl\n",
      "  Best params: {'hidden_layer_sizes': (200, 100), 'alpha': 0.01, 'learning_rate': 0.01, 'activation': 'relu'}\n",
      "  Validation RMSE: 4.2780, MAE: 3.3522, R²: 0.3592\n",
      "\n",
      "Outer Fold 4/5\n",
      "  Testing 48 hyperparameter combinations...\n",
      "    Tested 5/48 combinations...\n",
      "    Tested 10/48 combinations...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Run nested CV\u001b[39;00m\n\u001b[32m      2\u001b[39m start_time = time.time()\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m results = \u001b[43mnested_cv_evaluation_pytorch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpreprocessor_scaled\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparam_grid\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mouter_cv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43minner_cv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m512\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m200\u001b[39;49m\n\u001b[32m     13\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     15\u001b[39m total_time = time.time() - start_time\n\u001b[32m     16\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mTotal time: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_time/\u001b[32m60\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m minutes\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 66\u001b[39m, in \u001b[36mnested_cv_evaluation_pytorch\u001b[39m\u001b[34m(X, y, preprocessor, param_grid, outer_cv, inner_cv, device, batch_size, epochs)\u001b[39m\n\u001b[32m     58\u001b[39m     \u001b[38;5;66;03m# Create and train model\u001b[39;00m\n\u001b[32m     59\u001b[39m     model = NeuralNetwork(\n\u001b[32m     60\u001b[39m         input_size=input_size,\n\u001b[32m     61\u001b[39m         hidden_sizes=params[\u001b[33m'\u001b[39m\u001b[33mhidden_layer_sizes\u001b[39m\u001b[33m'\u001b[39m],\n\u001b[32m     62\u001b[39m         activation=params[\u001b[33m'\u001b[39m\u001b[33mactivation\u001b[39m\u001b[33m'\u001b[39m],\n\u001b[32m     63\u001b[39m         dropout=\u001b[32m0.0\u001b[39m\n\u001b[32m     64\u001b[39m     )\n\u001b[32m---> \u001b[39m\u001b[32m66\u001b[39m     _, val_loss = \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     67\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     68\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mlearning_rate\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     69\u001b[39m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43malpha\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     70\u001b[39m \u001b[43m        \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     71\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\n\u001b[32m     72\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     74\u001b[39m     inner_scores.append(val_loss)\n\u001b[32m     76\u001b[39m avg_inner_score = np.mean(inner_scores)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 15\u001b[39m, in \u001b[36mtrain_model\u001b[39m\u001b[34m(model, train_loader, val_loader, learning_rate, weight_decay, epochs, device)\u001b[39m\n\u001b[32m     13\u001b[39m model.train()\n\u001b[32m     14\u001b[39m train_loss = \u001b[32m0.0\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mX_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_batch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_batch\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_batch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_batch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/School/Year3Fall/CS363M/CowsFinalProject/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:732\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    729\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    730\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    731\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m732\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    733\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    734\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    735\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    736\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    737\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    738\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/School/Year3Fall/CS363M/CowsFinalProject/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:788\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    786\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    787\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m788\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    789\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    790\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/School/Year3Fall/CS363M/CowsFinalProject/.venv/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     50\u001b[39m         data = \u001b[38;5;28mself\u001b[39m.dataset.__getitems__(possibly_batched_index)\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m         data = [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 10\u001b[39m, in \u001b[36mCattleDataset.__getitem__\u001b[39m\u001b[34m(self, idx)\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__len__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m      8\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m.X)\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx):\n\u001b[32m     11\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     12\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.X[idx], \u001b[38;5;28mself\u001b[39m.y[idx]\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Run nested CV\n",
    "start_time = time.time()\n",
    "\n",
    "results = nested_cv_evaluation_pytorch(\n",
    "    X_train, y_train,\n",
    "    preprocessor_scaled,\n",
    "    param_grid,\n",
    "    outer_cv,\n",
    "    inner_cv,\n",
    "    device=device,\n",
    "    batch_size=512,\n",
    "    epochs=200\n",
    ")\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "print(f\"\\nTotal time: {total_time/60:.2f} minutes\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e9c4438",
   "metadata": {},
   "source": [
    "## Results Summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b424fcc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display results summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"RESULTS SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Average RMSE: {results['avg_rmse']:.4f} (±{results['std_rmse']:.4f})\")\n",
    "print(f\"Average MAE: {results['avg_mae']:.4f}\")\n",
    "print(f\"Average R²: {results['avg_r2']:.4f}\")\n",
    "\n",
    "print(\"\\nBest parameters for each fold:\")\n",
    "for i, params in enumerate(results['best_params']):\n",
    "    print(f\"\\nFold {i+1}:\")\n",
    "    for key, value in params.items():\n",
    "        print(f\"  {key}: {value}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c64d786e",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e56b920a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "048bd0f7",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e59bc63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load cleaned data\n",
    "TRAIN_PATH = \"cleaned_train.csv\"\n",
    "TEST_PATH = \"cleaned_test.csv\"\n",
    "\n",
    "train = pd.read_csv(TRAIN_PATH)\n",
    "test = pd.read_csv(TEST_PATH)\n",
    "\n",
    "print(f\"Train shape: {train.shape}\")\n",
    "print(f\"Test shape: {test.shape}\")\n",
    "print(f\"\\nTrain columns: {train.columns.tolist()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad608015",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce57cef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
